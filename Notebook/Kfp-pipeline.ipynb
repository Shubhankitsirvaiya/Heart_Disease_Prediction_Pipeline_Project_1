{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48f517f0-7aa3-45a7-8cbf-a93e1e99129f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Connection Successful!\n",
      "📝 Available Tables:      age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "0     63    1   3       145   233    1        0      150      0      2.3   \n",
      "1     37    1   2       130   250    0        1      187      0      3.5   \n",
      "2     41    0   1       130   204    0        0      172      0      1.4   \n",
      "3     56    1   1       120   236    0        1      178      0      0.8   \n",
      "4     57    0   0       120   354    0        1      163      1      0.6   \n",
      "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
      "298   57    0   0       140   241    0        1      123      1      0.2   \n",
      "299   45    1   3       110   264    0        1      132      0      1.2   \n",
      "300   68    1   0       144   193    1        1      141      0      3.4   \n",
      "301   57    1   0       130   131    0        1      115      1      1.2   \n",
      "302   57    0   1       130   236    0        0      174      0      0.0   \n",
      "\n",
      "     slope  ca  thal  target  \n",
      "0        0   0     1       1  \n",
      "1        0   0     2       1  \n",
      "2        2   0     2       1  \n",
      "3        2   0     2       1  \n",
      "4        2   0     2       1  \n",
      "..     ...  ..   ...     ...  \n",
      "298      1   0     3       0  \n",
      "299      1   0     3       0  \n",
      "300      1   2     3       0  \n",
      "301      1   1     3       0  \n",
      "302      1   1     2       0  \n",
      "\n",
      "[303 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "#  Test Connection Function\n",
    "def test_db_connection():\n",
    "    try:\n",
    "        engine = create_engine(\n",
    "            \"postgresql://pgadmin:MyPass06@ml-pipeline-pg-server.postgres.database.azure.com:5432/ml_pipeline_db\"\n",
    "        )\n",
    "        connection = engine.connect()\n",
    "        print(\" Connection Successful!\")\n",
    "        \n",
    "        #  Test Query\n",
    "        # query1 = \"SELECT table_name FROM information_schema.tables WHERE table_schema='public';\"\n",
    "        query = \"SELECT * FROM heart_disease_data;\"\n",
    "        tables = pd.read_sql(query, connection)\n",
    "        print(\"📝 Available Tables:\", tables)\n",
    "        \n",
    "        connection.close()\n",
    "    except Exception as e:\n",
    "        print(\" Connection Failed:\", e)\n",
    "\n",
    "# 🔥 Run the Test\n",
    "test_db_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75973d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import component, pipeline, InputPath, OutputPath, Condition\n",
    "import kfp.compiler as compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7f2ff36-f561-4173-b104-4b21a7f37d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# COMPONENT 1: Fetch Heart Disease Data from Azure and Split\n",
    "# -------------------------------------------------\n",
    "from kfp.v2.dsl import component, OutputPath\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"pandas\", \"sqlalchemy\", \"psycopg2\"]\n",
    ")\n",
    "def fetch_heart_data_from_azure(eval_path: OutputPath(\"Dataset\"), train_path: OutputPath(\"Dataset\")):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sqlalchemy import create_engine\n",
    "\n",
    "    try:\n",
    "        engine = create_engine(\n",
    "            \"postgresql://pgadmin:MyPass06@ml-pipeline-pg-server.postgres.database.azure.com:5432/ml_pipeline_db\"\n",
    "        )\n",
    "        df = pd.read_sql(\"SELECT * FROM heart_disease_data\", engine)\n",
    "        print(\"Fetched data from Azure PostgreSQL\")\n",
    "    except Exception as e:\n",
    "        print(\"Azure DB fetch failed, generating synthetic fallback data.\", e)\n",
    "        np.random.seed(42)\n",
    "        df = pd.DataFrame({\n",
    "            \"age\": np.random.randint(29, 77, 303),\n",
    "            \"sex\": np.random.choice([0, 1], 303),\n",
    "            \"cp\": np.random.randint(0, 4, 303),\n",
    "            \"trestbps\": np.random.randint(94, 200, 303),\n",
    "            \"chol\": np.random.randint(126, 564, 303),\n",
    "            \"fbs\": np.random.choice([0, 1], 303),\n",
    "            \"restecg\": np.random.randint(0, 2, 303),\n",
    "            \"thalach\": np.random.randint(71, 202, 303),\n",
    "            \"exang\": np.random.choice([0, 1], 303),\n",
    "            \"oldpeak\": np.round(np.random.uniform(0.0, 6.2, 303), 1),\n",
    "            \"slope\": np.random.randint(0, 3, 303),\n",
    "            \"ca\": np.random.randint(0, 5, 303),\n",
    "            \"thal\": np.random.randint(0, 4, 303),\n",
    "            \"target\": np.random.choice([0, 1], 303)\n",
    "        })\n",
    "\n",
    "    # Save first 100 rows as eval, full as train\n",
    "    df.head(100).to_csv(eval_path, index=False)\n",
    "    df.to_csv(train_path, index=False)\n",
    "    print(\"Saved 100-row eval set and full train set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0a3d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# COMPONENT 2: Check if Heart Disease Model Already Exists in MinIO\n",
    "# -------------------------------------------------\n",
    "from kfp.v2.dsl import component, OutputPath\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"boto3\"]\n",
    ")\n",
    "def check_heart_model_exists(status: OutputPath(str)):\n",
    "    import boto3\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=\"http://10.97.217.252:9000\",\n",
    "        aws_access_key_id=\"minio\",\n",
    "        aws_secret_access_key=\"minio123\"\n",
    "    )\n",
    "\n",
    "    response = s3.list_objects_v2(Bucket=\"mlpipeline\", Prefix=\"models/\")\n",
    "    print(\"Files in MinIO 'models/' bucket:\")\n",
    "    for obj in response.get(\"Contents\", []):\n",
    "        print(\" -\", obj[\"Key\"])\n",
    "\n",
    "    try:\n",
    "        s3.head_object(Bucket=\"mlpipeline\", Key=\"models/heart_model.pkl\")\n",
    "        result = \"exists\"\n",
    "        print(\"Model already exists in MinIO.\")\n",
    "    except Exception as e:\n",
    "        result = \"first_run\"\n",
    "        print(\"Model not found. First run detected.\")\n",
    "\n",
    "    with open(status, \"w\") as f:\n",
    "        f.write(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af508945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# COMPONENT 3: Evaluate Heart Disease Model from MinIO\n",
    "# -------------------------------------------------\n",
    "from kfp.v2.dsl import component, InputPath, OutputPath\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\", \"boto3\"]\n",
    ")\n",
    "def evaluate_heart_model(eval_path: InputPath(\"Dataset\"), result: OutputPath(str)):\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import boto3\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # MinIO client setup\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=\"http://10.97.217.252:9000\",\n",
    "        aws_access_key_id=\"minio\",\n",
    "        aws_secret_access_key=\"minio123\"\n",
    "    )\n",
    "\n",
    "    # Download model from MinIO\n",
    "    s3.download_file(\"mlpipeline\", \"models/heart_model.pkl\", \"heart_model.pkl\")\n",
    "    print(\"Model downloaded from MinIO\")\n",
    "\n",
    "    # Load eval dataset and model\n",
    "    df = pd.read_csv(eval_path)\n",
    "    X = df.drop(\"target\", axis=1)  # For heart disease, label column is 'target'\n",
    "    y = df[\"target\"]\n",
    "\n",
    "    model = joblib.load(\"heart_model.pkl\")\n",
    "    preds = model.predict(X)\n",
    "    acc = accuracy_score(y, preds)\n",
    "\n",
    "    print(f\"Eval Accuracy: {acc:.2f}\")\n",
    "\n",
    "    # Write evaluation result\n",
    "    with open(result, \"w\") as f:\n",
    "        f.write(\"good\" if acc >= 0.85 else \"bad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad823289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import component, InputPath\n",
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\", \"boto3\", \"prometheus_client\"]\n",
    ")\n",
    "def train_heart_model(train_path: InputPath(\"Dataset\")):\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import boto3\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from prometheus_client import Gauge, start_http_server\n",
    "    import threading\n",
    "    import time\n",
    "\n",
    "    # Start Prometheus server in a thread (non-blocking)\n",
    "    def start_prometheus():\n",
    "        start_http_server(8000)\n",
    "        while True:\n",
    "            time.sleep(1000)  # keep it alive\n",
    "\n",
    "    threading.Thread(target=start_prometheus, daemon=True).start()\n",
    "\n",
    "    # Train the model\n",
    "    df = pd.read_csv(train_path)\n",
    "    X = df.drop(\"target\", axis=1)\n",
    "    y = df[\"target\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(f\"Training complete - Accuracy: {acc:.2f}\")\n",
    "\n",
    "    # Save model locally\n",
    "    joblib.dump(model, \"model.pkl\")\n",
    "\n",
    "    # Export metric to Prometheus\n",
    "    acc_metric = Gauge('model_accuracy', 'Accuracy of the trained model')\n",
    "    acc_metric.set(acc)\n",
    "\n",
    "    # Upload model to MinIO\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=\"http://10.97.217.252:9000\",\n",
    "        aws_access_key_id=\"minio\",\n",
    "        aws_secret_access_key=\"minio123\"\n",
    "    )\n",
    "    s3.upload_file(\"model.pkl\", \"mlpipeline\", \"models/model.pkl\")\n",
    "    print(\"Model uploaded to MinIO\")\n",
    "    # Save to local\n",
    "    with open(\"accuracy.txt\", \"w\") as f:\n",
    "        f.write(str(acc))\n",
    "\n",
    "    # Upload to MinIO\n",
    "    s3.upload_file(\"accuracy.txt\", \"mlpipeline\", \"metrics/accuracy.txt\")\n",
    "    print(\"Uploaded accuracy.txt to MinIO (mlpipeline/metrics/accuracy.txt)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dc59a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"heart-disease-auto-retraining-pipeline\",\n",
    "    description=\"Retrains the heart disease model if accuracy drops on new data\"\n",
    ")\n",
    "def retraining_pipeline():\n",
    "    # Step 1: Fetch data from Azure PostgreSQL and split into eval/train\n",
    "    data = fetch_heart_data_from_azure()\n",
    "\n",
    "    # Step 2: Check if model already exists in MinIO\n",
    "    model_status_op = check_heart_model_exists()\n",
    "\n",
    "    # Step 3a: If no model exists, train the model\n",
    "    with dsl.If(model_status_op.outputs[\"status\"] == \"first_run\"):\n",
    "        train_heart_model(train_path=data.outputs[\"train_path\"])\n",
    "\n",
    "    # Step 3b: If model exists, evaluate it on new data\n",
    "    with dsl.If(model_status_op.outputs[\"status\"] == \"exists\"):\n",
    "        evaluation_op = evaluate_heart_model(eval_path=data.outputs[\"eval_path\"])\n",
    "\n",
    "        # Step 4: Retrain if evaluation result is bad (accuracy < threshold)\n",
    "        with dsl.If(evaluation_op.outputs[\"result\"] == \"bad\"):\n",
    "            train_heart_model(train_path=data.outputs[\"train_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69ae3ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# COMPILE THE PIPELINE\n",
    "# -------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    compiler.Compiler().compile(\n",
    "        pipeline_func=retraining_pipeline,\n",
    "        package_path=\"project_pipeline_v6.yaml\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995024d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
